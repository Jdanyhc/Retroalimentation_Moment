# -*- coding: utf-8 -*-
"""MIKAELA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Y-mxDBFiR9RO1V27QQeRBw9xswCn0t-

##First Part

dolwoad the respective libraries
"""

!pip install transformers

"""import the model for the sentiment analysis of huggiface"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

import pandas as pd

"""Open the file and select the lines of the document"""

f = open("/content/tiny_movie_reviews_dataset (1).txt", "r")
data = f.readlines()

data # print document for check correct import the data

import torch # import torch

for idx, line in enumerate(data, start=1): #enumerate respuest
  inputs = tokenizer(line, return_tensors="pt", truncation=True, max_length=512) #define the lenght of the lines and tokenize 
  with torch.no_grad():
    logits = model(**inputs).logits
    predicted_class_id = logits.argmax().item()
    if predicted_class_id == 1: #check the result of tests and classify
      print(str(idx) + ': POSITIVE')
    else:
      print(str(idx) + ': NEGATIVE')

"""##Second Part"""

from transformers import AutoTokenizer, AutoModelForTokenClassification

tokenizer_2 = AutoTokenizer.from_pretrained("dslim/bert-base-NER")

model_2 = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")

!pip install datasets

!pip install tokenizers

from datasets import load_dataset

dataset = load_dataset("wikiann", "bn")
# you can use any of the following config names as a second argument:
#"no", "ace", "af", "als", 
#"am", "an", "ang", "ar", 
#"arc", "arz", "as", "ast", 
#"ay", "az", "ba", "bar", 
#"be", "bg", "bh", "bn", 
#"bo", "br", "bs", "ca"
# [152 more config names available]

label_names = dataset["train"].features["ner_tags"].feature.names

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-multilingual-cased")

"""##Third part"""

es = open("/content/europarl-v7.es-en.es","r",encoding="utf-8") #lecture data of spanish text
doc_es = es.readlines()

prueb_es = doc_es[0:100] #select 100 cases

print(len(prueb_es))

en = open("/content/europarl-v7.es-en.en","r",encoding="utf-8") #lecture data of english text

doc_en = en.readlines()

prueb_en = doc_en[0:100] #select 100 cases

print(len(prueb_es))

import requests
import json

url = "https://deep-translate1.p.rapidapi.com/language/translate/v2"

for i in doc_es:
  payload = {
    "q": i,
    "source": "en",
    "target": "es"
  }
  headers = {
    "content-type": "application/json",
    "X-RapidAPI-Key": "7320366857msh355352b7ac27b15p129f27jsnb9c5af1e6ee5",
    "X-RapidAPI-Host": "deep-translate1.p.rapidapi.com"
  }

  response = requests.request("POST", url, json=payload, headers=headers)

  dictionary_2 = json.loads(response.text)
  print(dictionary_2["data"]["translations"]["translatedText"])

import requests
import json

url = "https://translo.p.rapidapi.com/api/v3/batch_translate"
for i in doc_en:
  payload = [
    {
      "from": "en",
      "to": "es",
      "text": i
    }
  ]
  headers = {
    "content-type": "application/json",
    "X-RapidAPI-Key": "7320366857msh355352b7ac27b15p129f27jsnb9c5af1e6ee5",
    "X-RapidAPI-Host": "translo.p.rapidapi.com"
  }

  response = requests.request("POST", url, json=payload, headers=headers)

  dictionary = json.loads(response.text)
  print(dictionary["batch_translations"][0]["text"])

from nltk.translate.bleu_score import sentence_bleu

score_ibm_bleu = sentence_bleu(doc_es[i].split(), dictionary["batch_translations"][0]["text"].split())